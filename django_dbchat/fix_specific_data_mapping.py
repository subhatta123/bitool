#!/usr/bin/env python3
"""
Fix Specific Data Mapping - Target the Correct Table
Fix the mapping between data source 'hu' and the actual Superstore data in DuckDB
"""

import os
import sys
import django

# Setup Django environment
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'dbchat_project.settings')
django.setup()

from datasets.models import DataSource, SemanticTable, SemanticColumn
from django.db import transaction
import duckdb

def fix_data_source_mapping():
    """Fix the data source mapping to point to the correct table"""
    
    print("üéØ FIXING DATA SOURCE MAPPING")
    print("=" * 50)
    
    # Get the data source
    try:
        data_source = DataSource.objects.get(name='hu')
        print(f"‚úÖ Found data source: {data_source.name} (ID: {data_source.id})")
    except DataSource.DoesNotExist:
        print("‚ùå Data source 'hu' not found")
        return False
    
    # Connect to DuckDB and get the correct table
    duckdb_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data_integration_storage', 'integrated_data.db')
    conn = duckdb.connect(duckdb_path)
    
    try:
        # The correct table with your data
        correct_table = 'source_csv_1_20250620_161848'
        
        print(f"üîç Examining table: {correct_table}")
        
        # Verify the table has data
        count_query = f"SELECT COUNT(*) FROM {correct_table}"
        count_result = conn.execute(count_query).fetchone()
        row_count = count_result[0] if count_result else 0
        
        print(f"üìä Table has {row_count} rows")
        
        if row_count == 0:
            print("‚ùå Table is empty")
            return False
        
        # Get table schema
        describe_query = f"DESCRIBE {correct_table}"
        columns = conn.execute(describe_query).fetchall()
        
        print(f"üìã Table has {len(columns)} columns:")
        for col in columns[:10]:
            print(f"   - {col[0]} ({col[1]})")
        if len(columns) > 10:
            print(f"   ... and {len(columns) - 10} more")
        
        # Load sample data for semantic layer creation
        print(f"\nüîß Loading data for semantic layer generation...")
        data_query = f"SELECT * FROM {correct_table} LIMIT 1000"
        data = conn.execute(data_query).fetchdf()
        
        print(f"‚úÖ Loaded {len(data)} rows for semantic analysis")
        
        # Update data source metadata to reflect the correct data
        print(f"üîÑ Updating data source metadata...")
        
        # Update the data source to reflect it contains Superstore data
        data_source.name = 'Superstore Dataset'  # More descriptive name
        
        # Update workflow status to indicate the correct table
        workflow_status = data_source.workflow_status or {}
        workflow_status['duckdb_table'] = correct_table
        workflow_status['row_count'] = row_count
        workflow_status['data_type'] = 'superstore'
        data_source.workflow_status = workflow_status
        
        data_source.save()
        
        print(f"‚úÖ Updated data source metadata")
        
        # Now create the semantic layer
        return create_semantic_layer_for_superstore(data_source, data, correct_table)
        
    finally:
        conn.close()

def create_semantic_layer_for_superstore(data_source, data, table_name):
    """Create semantic layer specifically for Superstore data"""
    
    print(f"\nüß† CREATING SEMANTIC LAYER FOR SUPERSTORE DATA")
    print("=" * 50)
    
    try:
        with transaction.atomic():
            # Delete any existing semantic layer first
            existing_semantic = SemanticTable.objects.filter(data_source=data_source)
            if existing_semantic.exists():
                print(f"üóëÔ∏è Removing existing semantic layer...")
                existing_semantic.delete()
            
            # Create new semantic table
            semantic_table_name = f"semantic_{data_source.id.hex.replace('-', '_')}"
            
            semantic_table = SemanticTable.objects.create(
                data_source=data_source,
                name=semantic_table_name,
                display_name='Superstore Sales Data',
                description=f'Semantic layer for Superstore sales data from DuckDB table {table_name}',
                business_purpose='Retail sales analytics including customer, product, and geographic data',
                is_fact_table=True,
                is_dimension_table=False,
                row_count_estimate=len(data)
            )
            
            print(f"‚úÖ Created semantic table: {semantic_table.name}")
            
            # Create semantic columns with business-friendly descriptions
            columns_created = 0
            
            # Define business-friendly column mappings for Superstore data
            superstore_column_mappings = {
                'Row_ID': ('identifier', 'Unique row identifier for each record'),
                'Order_ID': ('identifier', 'Unique order identifier'),
                'Order_Date': ('date', 'Date when the order was placed'),
                'Ship_Date': ('date', 'Date when the order was shipped'),
                'Ship_Mode': ('dimension', 'Shipping method used for delivery'),
                'Customer_ID': ('identifier', 'Unique customer identifier'),
                'Customer_Name': ('dimension', 'Name of the customer'),
                'Segment': ('dimension', 'Customer segment (Consumer, Corporate, Home Office)'),
                'Country': ('dimension', 'Country where the order was placed'),
                'City': ('dimension', 'City where the order was delivered'),
                'State': ('dimension', 'State where the order was delivered'),
                'Postal_Code': ('dimension', 'Postal code of delivery location'),
                'Region': ('dimension', 'Geographic region (West, East, Central, South)'),
                'Product_ID': ('identifier', 'Unique product identifier'),
                'Category': ('dimension', 'Product category (Furniture, Office Supplies, Technology)'),
                'Sub_Category': ('dimension', 'Product sub-category'),
                'Product_Name': ('dimension', 'Name of the product'),
                'Sales': ('measure', 'Sales amount in dollars'),
                'Quantity': ('measure', 'Number of items sold'),
                'Discount': ('measure', 'Discount percentage applied'),
                'Profit': ('measure', 'Profit amount in dollars')
            }
            
            for col_name in data.columns:
                try:
                    col_data = data[col_name]
                    
                    # Get semantic type and description from mapping
                    if col_name in superstore_column_mappings:
                        semantic_type, description = superstore_column_mappings[col_name]
                    else:
                        semantic_type = 'dimension'
                        description = f'Column {col_name} from Superstore dataset'
                    
                    # Get sample values (safe conversion)
                    sample_values = []
                    try:
                        non_null_values = col_data.dropna().head(3)
                        for val in non_null_values:
                            try:
                                sample_values.append(str(val))
                            except:
                                continue
                    except:
                        sample_values = []
                    
                    semantic_column = SemanticColumn.objects.create(
                        semantic_table=semantic_table,
                        name=col_name,
                        display_name=col_name.replace('_', ' ').title(),
                        description=description,
                        data_type=str(col_data.dtype),
                        semantic_type=semantic_type,
                        sample_values=sample_values,
                        is_nullable=bool(col_data.isnull().any()),
                        is_editable=True,
                        etl_enriched=False
                    )
                    
                    columns_created += 1
                    print(f"   ‚úÖ {col_name} ({semantic_type})")
                    
                except Exception as col_error:
                    print(f"   ‚ùå Failed to create column {col_name}: {col_error}")
                    continue
            
            # Update workflow status
            workflow_status = data_source.workflow_status or {}
            workflow_status['semantics_completed'] = True
            workflow_status['semantics_source'] = 'duckdb'
            workflow_status['semantics_table_name'] = semantic_table.name
            workflow_status['duckdb_table_name'] = table_name
            workflow_status['semantic_columns'] = columns_created
            data_source.workflow_status = workflow_status
            data_source.save()
            
            print(f"\nüéâ SUCCESS! Created semantic layer with {columns_created} columns")
            print(f"‚úÖ Data source now properly mapped to Superstore data")
            print(f"‚úÖ Semantic layer generation will now work correctly")
            print(f"‚úÖ No more CSV file dependency issues")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Error creating semantic layer: {e}")
        import traceback
        traceback.print_exc()
        return False

def verify_fix():
    """Verify that the fix worked"""
    
    print(f"\nüîç VERIFYING FIX")
    print("=" * 50)
    
    try:
        data_source = DataSource.objects.get(name='Superstore Dataset')
        print(f"‚úÖ Data source found: {data_source.name}")
        
        semantic_tables = SemanticTable.objects.filter(data_source=data_source)
        print(f"‚úÖ Semantic tables: {semantic_tables.count()}")
        
        if semantic_tables.exists():
            semantic_table = semantic_tables.first()
            columns = SemanticColumn.objects.filter(semantic_table=semantic_table)
            print(f"‚úÖ Semantic columns: {columns.count()}")
            
            print(f"\nüìä Sample columns:")
            for col in columns[:5]:
                print(f"   - {col.name} ({col.semantic_type}): {col.description}")
            
            return True
        else:
            print(f"‚ùå No semantic tables found")
            return False
            
    except Exception as e:
        print(f"‚ùå Verification failed: {e}")
        return False

if __name__ == "__main__":
    print("üéØ TARGETED FIX FOR DATA SOURCE MAPPING")
    print("=" * 70)
    
    # Step 1: Fix the data source mapping
    mapping_fixed = fix_data_source_mapping()
    
    if mapping_fixed:
        # Step 2: Verify the fix
        verification_passed = verify_fix()
        
        print(f"\n" + "=" * 70)
        print("üìä FIX RESULTS:")
        print(f"   üéØ Data source mapping: {'‚úÖ FIXED' if mapping_fixed else '‚ùå FAILED'}")
        print(f"   üîç Verification: {'‚úÖ PASSED' if verification_passed else '‚ùå FAILED'}")
        
        if mapping_fixed and verification_passed:
            print(f"\nüéâ COMPLETE SUCCESS!")
            print("‚úÖ Your data source now correctly maps to the Superstore data in DuckDB")
            print("‚úÖ Semantic layer generation will work properly")
            print("‚úÖ No more dataset confusion - you'll see the correct 9994 rows")
            print("‚úÖ The system now knows it's Superstore data, not Titanic data")
        else:
            print(f"\n‚ö†Ô∏è Fix incomplete - check error messages above")
    else:
        print(f"\n‚ùå Failed to fix data source mapping") 