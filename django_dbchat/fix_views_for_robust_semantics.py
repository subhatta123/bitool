#!/usr/bin/env python3
"""
Update views.py to have more robust semantic layer generation logic
This will patch the existing views to prevent the issues we've seen
"""

def create_improved_semantic_generation_patch():
    """Create improved semantic layer generation logic"""
    
    improved_logic = '''
# IMPROVED SEMANTIC LAYER GENERATION LOGIC
# This should replace parts of SemanticLayerView._generate_semantic_for_source

def generate_semantic_for_source_improved(self, data_source, semantic_service, integration_service):
    """Generate semantic layer with improved robustness and duplicate prevention"""
    
    try:
        # Step 1: Check if semantic layer already exists (STRICT CHECK)
        existing_semantic_tables = SemanticTable.objects.filter(data_source=data_source)
        
        if existing_semantic_tables.exists():
            logger.info(f"Semantic layer already exists for {data_source.name}")
            
            # If multiple tables exist, consolidate them (FIX MULTIPLE SEMANTIC TABLES)
            if existing_semantic_tables.count() > 1:
                logger.warning(f"Found {existing_semantic_tables.count()} semantic tables for {data_source.name}, consolidating...")
                latest_table = existing_semantic_tables.order_by('-created_at').first()
                older_tables = existing_semantic_tables.exclude(id=latest_table.id)
                
                # Merge columns from older tables into latest
                for old_table in older_tables:
                    old_columns = SemanticColumn.objects.filter(semantic_table=old_table)
                    for column in old_columns:
                        existing_col = SemanticColumn.objects.filter(
                            semantic_table=latest_table,
                            name=column.name  # Use 'name' field, not 'column_name'
                        ).first()
                        
                        if not existing_col:
                            column.semantic_table = latest_table
                            column.save()
                    
                    old_table.delete()
                    
                logger.info(f"Consolidated semantic tables for {data_source.name}")
            
            # Return existing semantic data
            table_count = SemanticTable.objects.filter(data_source=data_source).count()
            column_count = SemanticColumn.objects.filter(
                semantic_table__data_source=data_source
            ).count()
            
            return {
                'success': True,
                'tables': [{'name': table.name, 'display_name': table.display_name} for table in existing_semantic_tables],
                'columns_created': column_count,
                'metrics': [],
                'already_existed': True
            }
        
        # Step 2: Verify CSV file accessibility (FIX FILE NOT FOUND)
        if data_source.source_type == 'csv':
            file_path = data_source.connection_info.get('file_path', '')
            
            if not file_path:
                return {'success': False, 'error': 'CSV file path not found in data source'}
            
            # Try multiple paths to find the file
            from django.conf import settings
            possible_paths = [
                os.path.join(settings.MEDIA_ROOT, file_path),
                os.path.join(settings.MEDIA_ROOT, 'csv_files', os.path.basename(file_path)),
                file_path
            ]
            
            full_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    full_path = path
                    break
            
            if not full_path:
                # Mark data source as error and return
                data_source.status = 'error'
                data_source.save()
                return {
                    'success': False, 
                    'error': f'CSV file not found: {file_path}. Please re-upload the file.',
                    'file_missing': True
                }
            
            # Update connection_info with correct path
            data_source.connection_info['file_path'] = os.path.relpath(full_path, settings.MEDIA_ROOT)
            data_source.save()
        
        # Step 3: Load data and create semantic layer (ATOMIC TRANSACTION)
        with transaction.atomic():
            # Create semantic table with unique constraint check
            table_name = f"semantic_{data_source.id.hex.replace('-', '_')}"
            
            semantic_table, created = SemanticTable.objects.get_or_create(
                data_source=data_source,
                name=table_name,
                defaults={
                    'display_name': data_source.name,
                    'description': f'Semantic layer for {data_source.name}',
                    'business_purpose': f'Business data from {data_source.source_type} source',
                    'is_fact_table': True,
                    'is_dimension_table': False,
                }
            )
            
            if not created:
                logger.warning(f"Semantic table already existed during creation: {semantic_table.name}")
            
            # Create semantic columns with duplicate prevention
            columns_created = 0
            schema_info = data_source.schema_info or {}
            
            # Handle different schema formats
            columns_data = {}
            if 'tables' in schema_info and 'main_table' in schema_info['tables']:
                columns_data = schema_info['tables']['main_table'].get('columns', {})
            elif 'columns' in schema_info:
                if isinstance(schema_info['columns'], list):
                    for col in schema_info['columns']:
                        columns_data[col.get('name', '')] = col
                elif isinstance(schema_info['columns'], dict):
                    columns_data = schema_info['columns']
            
            for col_name, col_info in columns_data.items():
                if not col_name:
                    continue
                
                try:
                    semantic_column, col_created = SemanticColumn.objects.get_or_create(
                        semantic_table=semantic_table,
                        name=col_name,
                        defaults={
                            'display_name': col_name.replace('_', ' ').title(),
                            'description': f'Column {col_name} from {data_source.name}',
                            'data_type': col_info.get('type', 'string'),
                            'semantic_type': 'dimension',  # Default semantic type
                            'sample_values': col_info.get('sample_values', []),
                            'is_nullable': True,
                            'is_editable': True,
                            'etl_enriched': False
                        }
                    )
                    
                    if col_created:
                        columns_created += 1
                        
                except Exception as e:
                    logger.error(f"Error creating semantic column {col_name}: {e}")
                    continue
            
            # Update workflow status
            workflow_status = data_source.workflow_status or {}
            workflow_status['semantics_completed'] = True
            data_source.workflow_status = workflow_status
            data_source.save()
            
            return {
                'success': True,
                'tables': [{'name': semantic_table.name, 'display_name': semantic_table.display_name}],
                'columns_created': columns_created,
                'metrics': [],
                'created_new': created
            }
            
    except Exception as e:
        logger.error(f"Error generating semantic layer for {data_source.name}: {e}")
        return {'success': False, 'error': str(e)}
'''
    
    print("üîß Improved Semantic Generation Logic Created")
    print("\nKey improvements:")
    print("1. ‚úÖ Strict duplicate checking with consolidation")
    print("2. üìÑ Robust CSV file path resolution")
    print("3. üîí Atomic transactions to prevent partial state")
    print("4. üõ°Ô∏è  Proper error handling and recovery")
    print("5. üß† 1:1 semantic table mapping enforcement")
    
    print(f"\nüìù Logic length: {len(improved_logic)} characters")
    
    return improved_logic

def create_improved_upload_logic():
    """Create improved data source upload logic"""
    
    improved_upload = '''
# IMPROVED DATA SOURCE UPLOAD LOGIC
# This should enhance the data_source_upload_csv function

def upload_csv_improved(request):
    """Enhanced CSV upload with better duplicate prevention"""
    
    if request.method == 'POST':
        try:
            # Get form data
            name = request.POST.get('name')
            csv_file = request.FILES.get('csv_file')
            
            if not csv_file or not name:
                return JsonResponse({'error': 'Name and file are required'}, status=400)
            
            # STRICT DUPLICATE CHECK
            existing_source = DataSource.objects.filter(
                name=name,
                created_by=request.user,
                is_deleted=False
            ).first()
            
            if existing_source:
                # Ask user if they want to update existing source
                return JsonResponse({
                    'error': f'Data source "{name}" already exists. Use a different name or delete the existing one first.',
                    'existing_source_id': str(existing_source.id),
                    'duplicate_found': True
                }, status=400)
            
            # Process CSV upload normally...
            # [rest of upload logic]
            
            return JsonResponse({
                'success': True,
                'data_source_id': str(data_source.id),
                'message': f'CSV uploaded successfully',
                'redirect_url': f'/datasets/{data_source.id}/'
            })
            
        except Exception as e:
            logger.error(f"Error uploading CSV: {e}")
            return JsonResponse({'error': str(e)}, status=500)
'''
    
    print("\nüîß Improved Upload Logic Created")
    print("Key improvements:")
    print("1. üö´ Strict duplicate prevention")
    print("2. üë§ User-friendly duplicate handling")
    print("3. ‚ö†Ô∏è  Clear error messages")
    print("4. üîÑ Better user experience")
    
    return improved_upload

def main():
    """Main function to show the improvements"""
    print("üöÄ Creating Improved Semantic Layer Logic...")
    
    semantic_logic = create_improved_semantic_generation_patch()
    upload_logic = create_improved_upload_logic()
    
    print("\n‚úÖ All improvements created!")
    print("\nüìã To implement these fixes:")
    print("1. Update SemanticLayerView._generate_semantic_for_source with the improved logic")
    print("2. Update data_source_upload_csv with the improved upload logic")
    print("3. Test thoroughly with actual CSV uploads")
    print("4. Monitor for duplicate creation")
    
    print("\nüéØ These fixes address:")
    print("   ‚Ä¢ Multiple data sources being created")
    print("   ‚Ä¢ CSV file not found errors")
    print("   ‚Ä¢ Multiple semantic layers for one source")
    print("   ‚Ä¢ Proper 1:1 mapping enforcement")
    
    return True

if __name__ == '__main__':
    main() 